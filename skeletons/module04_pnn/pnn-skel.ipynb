{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546b6be0-6f8a-40e0-921b-72b180e1b9c9",
   "metadata": {},
   "source": [
    "# Probabilistic Neural Networks Demo\n",
    "\n",
    "## Advanced Machine Learning\n",
    "\n",
    "### Andrew H. Fagg (andrewhfagg@gmail.com)\n",
    "\n",
    "Key versions:\n",
    "- python=3.12\n",
    "- tensorflow=2.18.0\n",
    "- tf_keras=2.18.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b83a7c-4d49-4cbc-97fa-1d4f29f22e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it is really important to import the version of keras that\n",
    "#  is compatible with TF Probability\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# THIS IS REALLY IMPORTANT\n",
    "import tf_keras as keras\n",
    "from tf_keras.models import Sequential\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Sub namespaces that are useful later\n",
    "# Tensorflow Distributions\n",
    "tfd = tfp.distributions\n",
    "# Probability Layers \n",
    "tfpl = tfp.layers\n",
    "\n",
    "from tf_keras.layers import Layer, Concatenate\n",
    "from tf_keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tf_keras import Input, Model\n",
    "from matplotlib import colors\n",
    "from tf_keras.utils import plot_model\n",
    "\n",
    "#################################################################\n",
    "# Default plotting parameters\n",
    "FIGURESIZE=(8,6)\n",
    "FONTSIZE=14\n",
    "\n",
    "plt.rcParams['figure.figsize'] = FIGURESIZE\n",
    "plt.rcParams['font.size'] = FONTSIZE\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = FONTSIZE\n",
    "plt.rcParams['ytick.labelsize'] = FONTSIZE\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203b41f-5358-44c9-b447-613fc243e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the GPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa151a1-7a06-4bd4-aa18-baeba5af433c",
   "metadata": {},
   "source": [
    "## Create Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474af60-f328-437f-aa2e-ebae1ff7c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~Gaussian density\n",
    "def gauss(t, mu, sigma):\n",
    "    return np.exp(-np.square((t-mu)/sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47860a7b-db1b-4927-adc5-5cbb277e979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular sampling\n",
    "t = np.arange(0, 10, .01)\n",
    "\n",
    "# Create noisy values that are periodic in t\n",
    "y = np.cos(t) + t/3\n",
    "noise = np.random.normal(0, .1*t, y.shape[0])\n",
    "y = y + noise\n",
    "\n",
    "# Create a 2nd set of noisy values that have two modes centered at t=3\n",
    "g = gauss(t, 3, 1)\n",
    "sign = np.random.choice([1,-1], size = y.shape[0])\n",
    "y2 = y+3*sign*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a1002-1fe7-4bad-a631-1e154c3306cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y, 'c.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876544f-1f15-46e5-93ed-5cf9da426db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8975e-a811-49c0-8b4d-863360afdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2)  \n",
    "\n",
    "t_example = 6\n",
    "epsilon = 0.2\n",
    "\n",
    "axes[1,0].plot(t, y, 'c.')\n",
    "axes[1,0].plot([t_example,t_example], [-1,5.5], 'r--')\n",
    "axes[1,0].set_ylim([-1,6])\n",
    "\n",
    "axes[1,0].set_xlabel('t')\n",
    "axes[1,0].set_ylabel('y')\n",
    "\n",
    "axes[1,1].hist(y, bins=41, orientation='horizontal')\n",
    "axes[1,1].set_ylim([-1,6])\n",
    "axes[0,0].hist(t, bins=41)\n",
    "\n",
    "# Pick ys around t=6\n",
    "ys = [y_sample for t_sample, y_sample in zip(t,y) \n",
    "      if (t_sample > t_example-epsilon) and (t_sample < t_example+epsilon)]\n",
    "axes[0,1].hist(ys, bins=41, orientation='horizontal')\n",
    "axes[0,1].set_ylim([-1,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0629d2-bf3d-404a-b910-cc9b2862ea0d",
   "metadata": {},
   "source": [
    "## Divergent Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cf958-8923-4bab-be4c-a75be1ca8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,y2, 'c.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab2050-5da3-4ffb-a8d1-7b75fc4024f7",
   "metadata": {},
   "source": [
    "## Create network\n",
    "\n",
    "Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b73d2-e64b-4e24-8664-d88ae2d0b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_stack(n_inputs:int, \n",
    "                          n_hidden:int, \n",
    "                          n_output:int, \n",
    "                          activation:str='elu', \n",
    "                          activation_out:str=None,\n",
    "                          lrate:float=0.001, \n",
    "                          dropout:float=None, \n",
    "                          dropout_input:float=None, \n",
    "                          kernel_regularizer_L2:float=None, \n",
    "                          kernel_regularizer_L1:float=None):\n",
    "\n",
    "    '''\n",
    "    General network building code that creates a stack of Dense layers.\n",
    "    The output of the stack is a list of output Keras Tensors, each with its own activation function.\n",
    "    If the output activation function is 'positive', then the output is elu()+1.1\n",
    "    \n",
    "    :param n_inputs: Number of input units\n",
    "    :param n_hidden: List of hidden layer sizes\n",
    "    :param n_output: List of the number of output units for each output Tensor\n",
    "    :param activation: Activation function for hidden layers\n",
    "    :param activation_out: List of the activation functions for each output Tensor\n",
    "    :param lrate: Learning rate\n",
    "    :param dropout: Dropout probability for hidden layers\n",
    "    :param dropout_input: Dropout probability for input layer\n",
    "    :param kernel_regularizer_L2: L2 regularization param\n",
    "    :param kernel_regularizer_L1: L1 regularization param\n",
    "    :return: (input tensor, list of output tensors)\n",
    "    '''\n",
    "    \n",
    "    if dropout is not None:\n",
    "        print(\"DENSE: DROPOUT %f\"%dropout)\n",
    "\n",
    "    # L2 or L1 regularization?\n",
    "    kernel_regularizer = None\n",
    "    if kernel_regularizer_L2 is not None:\n",
    "        print(\"DENSE: L2 Regularization %f\"%kernel_regularizer)\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(kernel_regularizer)\n",
    "    elif kernel_regularizer_L1 is not None:\n",
    "        # Only us L1 if specified *and* L2 is not active\n",
    "        print(\"DENSE: L1 Regularization %f\"%kernel_regularizer_L1)\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(kernel_regularizer_L1)\n",
    "\n",
    "    # Input layer\n",
    "    input_tensor = tensor = Input(shape=(n_inputs,))\n",
    "    \n",
    "    # Dropout input features?\n",
    "    if dropout_input is not None:\n",
    "        tensor = Dropout(rate=dropout_input, name=\"dropout_input\")(tensor)\n",
    "            \n",
    "    # Loop over hidden layers\n",
    "    for i, n in enumerate(n_hidden):             \n",
    "        tensor = Dense(n, use_bias=True, name=\"hidden_%02d\"%(i), activation=activation,\n",
    "                 kernel_regularizer=kernel_regularizer)(tensor)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            tensor = Dropout(rate=dropout, name=\"dropout_%02d\"%(i))(tensor)\n",
    "    \n",
    "    # Output layers\n",
    "    outputs = []\n",
    "    for i, (n, act) in enumerate(zip(n_output, activation_out)):\n",
    "        o = Dense(n, use_bias=True, name=\"output%d\"%i, activation=act)(tensor)\n",
    "        outputs.append(o)\n",
    "\n",
    "    return input_tensor, outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8da96-0d19-4659-939e-b3b7af11a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple network\n",
    "input_tensor, output_tensors  = fully_connected_stack(n_inputs=1, \n",
    "                                                   n_hidden=#TODO\n",
    "                                                   n_output=#TODO,\n",
    "                                                   activation=#TODO,\n",
    "                                                   activation_out=#TODO,\n",
    "                                                   dropout=None)\n",
    "\n",
    "# Optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, amsgrad=False)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensors)\n",
    "model.compile(loss='mse', optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db2b41-e9e9-4473-a79b-ea8b3bd299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a251171-2be2-4538-a0d0-8dedfbbf8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(t, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99ec33-8ddc-4433-826c-12ee53a9dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbc421-4bac-417c-b514-efb4101f8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y, 'c.')\n",
    "plt.plot(t, y_hat, 'r')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce060bd9-187c-4341-acfd-1992cec543b4",
   "metadata": {},
   "source": [
    "## Probabilistic Outputs\n",
    "\n",
    "Next model: given the input (t), our model will generate the parameters for \n",
    "a Normal distribution.\n",
    "\n",
    "We will take an inner/outer model approach:\n",
    "- The inner model takes t as an input and produces mu and sigma as output.  \n",
    "- Outer model takes t as an input and produces a TF Probability Distribution (yes, it is not a Keras Tensor!).  We will use this outer model for training using a custom loss function that returns the negative log likelihood of the desired output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d5228-6c7c-4ccb-939f-2add3a42a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "\n",
    "@tf.function\n",
    "def mdn_loss(y, dist):\n",
    "    '''\n",
    "    Compute negative log likelihood of the desired output\n",
    "    \n",
    "    :param y: True value (from the training set)\n",
    "    :param dist: A TF Probability Distribution\n",
    "    :return: The negative likelihood of each true value\n",
    "    '''\n",
    "    return #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc346a-063f-45b9-8ccd-1f56f34cc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (t) is 1-dimensional\n",
    "n_inputs = 1\n",
    "\n",
    "# Output (y) is a 1-dimensional value\n",
    "d_output = 1\n",
    "\n",
    "\n",
    "# Inner model.  The outputs will be mu and sigma, respectively\n",
    "\n",
    "# Compute the number of output units\n",
    "n_outputs = tfpl.IndependentNormal.params_size(event_shape=d_output)\n",
    "\n",
    "# Main model stack\n",
    "input_tensor, output_tensors  = fully_connected_stack(n_inputs=n_inputs, \n",
    "                                                      n_hidden=[100, 50, 20], \n",
    "                                                      n_output=[n_outputs],\n",
    "                                                      activation='elu',\n",
    "                                                      activation_out=['linear'],\n",
    "                                                      dropout=None)\n",
    "\n",
    "model_inner = Model(inputs=input_tensor, outputs=output_tensors)\n",
    "\n",
    "#######################\n",
    "# Outer model\n",
    "# Key: do not mix Keras Tensors between these two models, but\n",
    "#  we can use the inner model as a layer\n",
    "\n",
    "tensor = input_tensor2 = Input(shape=(n_inputs,))\n",
    "output_tensors2 = model_inner(tensor)\n",
    "\n",
    "# Combine outputs into one tensor\n",
    "#output_tensors2 = Concatenate(axis=-1)(output_tensors2)\n",
    "\n",
    "# This layer takes a Keras Tensor as input and returns a TF Probability Distribution\n",
    "#  It also handles the constraint that std must be positive, so model_inner does not \n",
    "#  need to enforce this\n",
    "output2 = #TODO\n",
    "\n",
    "model_outer = Model(inputs=input_tensor2, outputs=output2)\n",
    "\n",
    "# Optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, amsgrad=False)\n",
    "\n",
    "# We don't have to use a built-in loss function; instead we provide our own\n",
    "model_outer.compile(optimizer=opt, loss=mdn_loss)\n",
    "\n",
    "print(model_outer.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2652f8c-355c-4350-b35c-faa453138869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model_outer, to_file='pnn_outer.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802242f-fcf7-47fa-a5d0-6faed59aa3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model_inner, to_file='pnn_inner.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b764e7f-5cbc-4890-9117-cd2f92c3f20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history2 = model_outer.fit(x=t, y=y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efc588-e2cf-4825-886c-58fa6d096fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the details of the distribution for each value of t\n",
    "\n",
    "# Approach #1: use the distributions generated by the outer model\n",
    "dists = #TODO\n",
    "mu = #TODO\n",
    "std = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfbd7d-5f51-476a-89c4-414add2a89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #2 (alternative to the above cell): use the outputs from the inner model\n",
    "\n",
    "params = #TODO\n",
    "mu = #TODO\n",
    "# The distribution includes the softplus nonlinearity to ensure that \n",
    "#  std is not negative.  We have to do it explicitly here\n",
    "std = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da219869-3e72-4622-be20-eae19a44b329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20e7bd-474a-4f28-a8e8-1945919e1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data and the distribution\n",
    "plt.plot(t, y, 'c.')\n",
    "plt.plot(t, mu, 'b-')\n",
    "plt.plot(t, mu+std, 'b--')\n",
    "plt.plot(t, mu-std, 'b--')\n",
    "plt.ylim([-7,7])\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2536eb-a4a3-4d5b-829d-5de5bfa480a6",
   "metadata": {},
   "source": [
    "## Fit to the Divergent Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148f1f9-7a7f-44cc-b2b8-e0a813b01bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model_outer.fit(x=t, y=y2, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036de34-be44-4dff-93bc-6c475caae683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Approach #1 \n",
    "dists = #TODO\n",
    "mu3 = #TODO\n",
    "std3 = #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e5b28-d28f-4f7e-b7c5-2cb265947d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for a specific input\n",
    "t_value = 3\n",
    "\n",
    "query = np.ones((20,)) * t_value\n",
    "samples3 = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac8d4d-0283-4695-a0c7-bea1c6101ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data and the distribution\n",
    "plt.plot(t, y2, 'c.')\n",
    "plt.plot(t, mu3, 'b')\n",
    "plt.plot(t, mu3+std3, 'b--')\n",
    "plt.plot(t, mu3-std3, 'b--')\n",
    "plt.plot(query, samples3, 'r.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217b2cd-6369-4d6f-b150-5f015c969cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff7f9a6-6393-4021-a032-fb2b4435ee54",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "- Inner model: compute the parameters of your mixture distribution.  For 2 Gaussians, we need 2 means, 2 standard deviations, and 2 weights.  The ultimate non-linearity for the weights is softmax(); our inner model will produce the values *before* the softmax (these are commonly referred to as 'logits')\n",
    "- Outer model: takes these distribution parameters as input into the Mixture Distribution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939a777-181d-46ad-a630-74ab16666992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is 1-dimensional\n",
    "n_inputs = 1\n",
    "\n",
    "# Output is a 1-dimensional value\n",
    "d_output = 1\n",
    "\n",
    "# Number of Gaussians\n",
    "n_gaussians = 2\n",
    "\n",
    "# Inner model.  The outputs will be mu, sigma, and logits, respectively\n",
    "\n",
    "# Compute the number of output units for the inner model\n",
    "n_outputs = tfpl.MixtureNormal.params_size(num_components=n_gaussians, \n",
    "                                           event_shape=d_output)\n",
    "\n",
    "# Main model stack\n",
    "input_tensor, output_tensors  = fully_connected_stack(n_inputs=n_inputs, \n",
    "                                                      n_hidden=[1000,100, 50, 20], \n",
    "                                                      n_output=[n_outputs],\n",
    "                                                      activation='elu',\n",
    "                                                      activation_out=['linear'],\n",
    "                                                      dropout=None)\n",
    "\n",
    "model_inner4 = Model(inputs=input_tensor, outputs=output_tensors)\n",
    "\n",
    "#######################\n",
    "# Outer model\n",
    "# Key: do not mix Keras Tensors between these two models, but\n",
    "#  we can use the inner model as a layer in the Outer model\n",
    "\n",
    "tensor = input_tensor2 = Input(shape=(n_inputs,))\n",
    "output_tensors2 = model_inner4(tensor)\n",
    "\n",
    "# This layer takes a Keras Tensor as input and returns a TF Probability Distribution\n",
    "#  It also handles the constraint that std must be positive\n",
    "#  NOTE: this only workks right now when using Keras 2\n",
    "output2 = #TODO\n",
    "\n",
    "model_outer4 = Model(inputs=input_tensor2, outputs=output2)\n",
    "\n",
    "# Optimizer\n",
    "opt4 = keras.optimizers.Adam(learning_rate=0.0001, amsgrad=False)\n",
    "\n",
    "# We don't have to use a built-in loss function.  Instead, we use the\n",
    "#   one defined above\n",
    "model_outer4.compile(optimizer=opt4, loss=mdn_loss)\n",
    "\n",
    "print(model_outer4.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee423a-9f6e-47fb-be2c-0759ee153122",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_outer4, to_file='pnn_mixture_outer.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794ce79-fa61-4425-946c-76b82ecfb8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model_inner4, to_file='pnn_mixture_outer.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc05161-4feb-4b88-b4f5-a936981b2246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need a lot more epochs to train this model\n",
    "history4 = model_outer4.fit(x=t, y=y2, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff2f82-53bb-4988-a346-2379623a65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show loss\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.ylabel('loss (sum negative likelihood)')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03218f65-0555-4cbd-8edf-9f5b194633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the model: Approach #1\n",
    "\n",
    "# The process is different than with a single Normal distribution\n",
    "\n",
    "# One parameterized dist for every element in t\n",
    "dists4 = #TODO\n",
    "\n",
    "# Extract the logits and convert into weights\n",
    "logits4 = dists4.tensor_distribution.mixture_distribution.logits.numpy() \n",
    "prob4 = #TODO\n",
    "\n",
    "# Means\n",
    "mu4 = dists4.tensor_distribution.components_distribution.tensor_distribution.mean().numpy() \n",
    "\n",
    "# Standard deviations (note that the PDF ensures that these stds are non-negative (it uses a softplus() nonlinearity\n",
    "std4 = dists4.tensor_distribution.components_distribution.tensor_distribution.stddev().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ebcfb-14e2-4929-b55b-7cb9f6398004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb25e21-7c56-493a-ada4-bcf8149ef945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a set of samples for a instant input\n",
    "t_value = 3\n",
    "query4 = np.ones((20,)) * t_value\n",
    "\n",
    "# Do the sampling\n",
    "samples4 = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c3080-f10a-466e-b69f-dbf81bc0e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data and the distribution\n",
    "plt.plot(t, y2, 'c.')\n",
    "\n",
    "# Two means\n",
    "plt.plot(t,mu4[:,0], 'b')\n",
    "plt.plot(t, mu4[:,1], 'r')\n",
    "\n",
    "# +/- one standard deviation\n",
    "plt.plot(t, mu4[:,0]+std4[:,0], 'b--')\n",
    "plt.plot(t, mu4[:,0]-std4[:,0], 'b--')\n",
    "\n",
    "plt.plot(t, mu4[:,1]+std4[:,1], 'r--')\n",
    "plt.plot(t, mu4[:,1]-std4[:,1], 'r--')\n",
    "\n",
    "# Show the weights\n",
    "plt.plot(t, prob4[:,0]-6, 'b')\n",
    "plt.plot(t, prob4[:,1]-6, 'r')\n",
    "\n",
    "# Sample distribution\n",
    "plt.plot(query4, samples4, 'k.')\n",
    "\n",
    "plt.ylim([-7,7])\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95859f1-9ef2-44d6-a562-15aebb828386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d106c-81d9-443c-ab0b-8dbac12d1a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343dec3f-2d7e-47cb-9a36-4dbd5071bccd",
   "metadata": {},
   "source": [
    "# Probabilistic Neural Networks Demo, Example II\n",
    "## A More General Approach (maybe)\n",
    "\n",
    "## Advanced Machine Learning\n",
    "\n",
    "### Andrew H. Fagg (andrewhfagg@gmail.com)\n",
    "\n",
    "The following approach does not require a Keras model to return a distribution.  On the one hand, the approach should work for a larger range of Keras implementations.  However, it is a little more wonky because:\n",
    "1. The 'desired output' is an input to the training model\n",
    "2. If we want to ask questions after training about the distributions, we have to construct them ourselves, rather than the outer model producing them\n",
    "\n",
    "\n",
    "Structure:\n",
    "- We are using a single Normal distribution\n",
    "- The inner model translates some input (t in this case)  into two outputs: mu and sigma for each input element\n",
    "- The outer model is used for training:\n",
    "   1. Inputs: t and the true observations (y)\n",
    "   2. Outputs: mu and sigma for each input element (for some reason, this version of Keras does not like having no output)\n",
    "   3. The model uses model.add_loss() directly, and does not declare a loss at the model.compile() step\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2d6fc-a42d-47e7-bb35-5613b9a6401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mdn_cost(parameter_tensors, y):\n",
    "    '''\n",
    "    Custom loss function\n",
    "    \n",
    "    Use this through model.add_loss()\n",
    "    Key here: this is evaluated using TF Tensors (as opposed to Keras Tensors)\n",
    "        The difference is that TF Tensors actually have values\n",
    "    :param parameter_tensors: List of TF tensors: one for mean, and the other for standard deviation\n",
    "    :param y: TF Tensor containing the true value that the likelihood is measured for\n",
    "    :return: Scalar mean negative log likelihood\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Extract the individual parameter tensors\n",
    "    mu, sigma = parameter_tensors\n",
    "\n",
    "    # Construct the distributions (one for each mu/sigma)\n",
    "    dist = tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "    # Mean log likelihood\n",
    "    return tf.reduce_mean(-dist.log_prob(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760e9a6-86c6-4394-ba7c-f62c5b655274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is 1-dimensional\n",
    "n_inputs = 1\n",
    "\n",
    "# Output is a 1-dimensional value\n",
    "d_output = 1\n",
    "\n",
    "# Build a simple network\n",
    "# Note that we have to explicitly force std to be positive\n",
    "input_tensor, output_tensors  = fully_connected_stack(n_inputs=1, \n",
    "                                                   n_hidden=[100, 50, 20], \n",
    "                                                   n_output=[d_output, d_output],\n",
    "                                                   activation='elu',\n",
    "                                                   activation_out=['linear', 'softplus'],\n",
    "                                                   dropout=None)\n",
    "\n",
    "\n",
    "model_inner5 = Model(inputs=input_tensor, outputs=output_tensors)\n",
    "\n",
    "####################################\n",
    "\n",
    "# Wrapper network has two inputs and two outputs\n",
    "#  A bit of a hack: the true values are inputs\n",
    "\n",
    "tensor = input_tensor5 = Input(shape=(n_inputs,))\n",
    "y_real = Input(shape=(n_inputs,))\n",
    "\n",
    "output_tensors5 = model_inner5(tensor)\n",
    "\n",
    "model_outer5 = #TODO\n",
    "\n",
    "# We can add any loss we want to our cost function\n",
    "model_outer5.add_loss(#TODO)\n",
    "\n",
    "# Default optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, amsgrad=False)\n",
    "\n",
    "# We don't have to use a built-in loss function\n",
    "model_outer5.compile(optimizer=opt)\n",
    "\n",
    "print(model_outer5.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a988c-16e5-4dba-9a4a-56e1650d1a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history5 = model_outer5.fit(x=[t,y], epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35703e0-5c14-4e50-99d3-dd2871432195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu5, std5 = model_inner5.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd2dae-ce15-4000-ab9d-3d7219ebd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for a specific input\n",
    "t_value = 7\n",
    "\n",
    "n_samples = 20\n",
    "query5 = np.ones((n_samples,)) * t_value\n",
    "mu_sample5, sigma_sample5 = model_inner5.predict([t_value])\n",
    "samples5 =  tfp.distributions.Normal(loc=mu_sample5, scale=sigma_sample5).sample([n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18c45c-29ab-4736-b8ce-5dbb986f9653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795bb3c-7bde-4cd5-8257-4b71a9064d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data and the distribution\n",
    "plt.plot(t, y, 'c.')\n",
    "plt.plot(t, mu5, 'b')\n",
    "plt.plot(t, mu5+std5, 'b--')\n",
    "plt.plot(t, mu5-std5, 'b--')\n",
    "plt.plot(query5, np.squeeze(samples5), 'r.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdf034-28b6-4102-996f-67f6476aa35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
